{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#README\n",
    "\n",
    "#below is the code for each aspect of the project\n",
    "#each cell has a title describing it's purpose and as such how it should be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining replays - this code was used to create the dataset\n",
    "#this does not need to be run as a hard copy already exists\n",
    "\n",
    "#dates for past seasons obtained from https://www.esportstales.com/rocket-league/competitive-season-end-date\n",
    "\n",
    "#API token unique to my ballchasing.com account\n",
    "token = \"rdJZjDqqQh5F8xUGbyo8crfZTCXVA3rPfdlGuyK5\"\n",
    "\n",
    "#base url used in API requests\n",
    "base_url = \"https://ballchasing.com/api\"\n",
    "\n",
    "#finding all the stats that haven't been averaged over the duration of the game and dividing them by the game\n",
    "#time (in mins, follows since this is how boost is averaged and the comparison with other stats that are already)\n",
    "#averaged is unchanged\n",
    "\n",
    "from pandas import json_normalize\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def sort_and_average(stats):\n",
    "    players = []\n",
    "    duration = stats['duration']\n",
    "    for a in stats['blue']['players']:\n",
    "        players.append(a['stats'])\n",
    "    for b in stats['orange']['players']:\n",
    "        players.append(b['stats'])\n",
    "    key_stats = json_normalize(players)\n",
    "    for s in key_stats.columns:\n",
    "        if (\"avg\" in s) or (\"percent\" in s) or (\"pm\" in s) or (\"mvp\" in s):\n",
    "            continue\n",
    "        else:\n",
    "            key_stats[s] /= (duration/60)\n",
    "    for i in range(len(key_stats['core.mvp'])):\n",
    "        key_stats.loc[i]['core.mvp'] = int(key_stats['core.mvp'][i])\n",
    "    return key_stats\n",
    "\n",
    "\n",
    "#function that returns the next date to look replays up until\n",
    "def next_date(date, season):\n",
    "    #list of dates 2 weeks after start of each season of 3 most recent complete seasons\n",
    "    current_seas = season\n",
    "    new_seas = season\n",
    "    key_dates_str = [\"2022-06-29\", \"2022-09-21\", \"2022-12-21\"]\n",
    "    key_dates = []\n",
    "    #converting key_dates to datetime objects\n",
    "    for d in key_dates_str:\n",
    "        dmy = datetime.datetime(int(d[0:4]), int(d[5:7]), int(d[8:10]))\n",
    "        key_dates.append(dmy)\n",
    "    date_str = \"T00:00:00Z\"\n",
    "    #baseline date according to the season being looked at\n",
    "    baseline = key_dates[int(current_seas[1])-7]\n",
    "    #converting date to datetime object\n",
    "    date = datetime.datetime(int(date[0:4]), int(date[5:7]), int(date[8:10]))\n",
    "    pos_date = date+datetime.timedelta(days=-18)\n",
    "    #checking if new date falls within 2 weeks of start of season\n",
    "    #if it does, reset dates to start looking for matches from end of previous season\n",
    "    if (baseline>pos_date) == True:\n",
    "        new_seas='f'+str(int(current_seas[1])-1)\n",
    "        before_date = baseline+datetime.timedelta(days=-15)\n",
    "        after_date = str(before_date+datetime.timedelta(days=-18))[:10]+date_str\n",
    "        before_date = str(before_date)[:10]+date_str\n",
    "        print(f\"New season: {new_seas}\")\n",
    "    #if not, subtract 2 days from after date and input date becomes before date, season is unchanged\n",
    "    else:\n",
    "        before_date = str(date)[:10]+date_str\n",
    "        after_date = str(pos_date)[:10]+date_str\n",
    "    #returning new pair of dates along with the season being looked in \n",
    "    return before_date, after_date, new_seas\n",
    "\n",
    "#date=\"2022-07-01\"\n",
    "#season='f9'\n",
    "#print(date, next_date(date, season))\n",
    "\n",
    "ranks_to_remove = [\"bronze-1\", \"bronze-2\", \"bronze-3\", \"silver-1\", \"silver-2\", \"silver-3\"]\n",
    "ranks = [\"platinum-1\", \"platinum-2\", \"platinum-3\", \"diamond-1\", \"diamond-2\", \"diamond-3\", \"champion-1\", \"champion-2\", \"champion-3\", \"grand-champion-1\", \"grand-champion-2\", \"grand-champion-3\", \"supersonic-legend\"]\n",
    "ranks_done = [\"gold-1\", \"gold-2\", \"gold-3\"]\n",
    "dates_to_ignore = [\"2020-12-23T00:00:00Z\", \"2021-04-07T00:00:00Z\", \"2021-04-21T00:00:00Z\", \"2021-08-11T00:00:00Z\", \"2021-08-25T00:00:00Z\", \"2021-11-17T00:00:00Z\", \"2021-12-01T00:00:00Z\", \"2022-03-09T00:00:00Z\", \"2022-03-23T00:00:00Z\", \"2022-06-15T00:00:00Z\", \"2022-06-29T00:00:00Z\", \"2022-09-07T00:00:00Z\", \"2022-09-21T00:00:00Z\", \"2023-01-01T00:00:00Z\"]\n",
    "replay_list = []\n",
    "ranks_done = 0\n",
    "season = 'f9'\n",
    "starting_date = \"2023-03-07T00:00:00Z\" #final full day before season ended \n",
    "\n",
    "#main loop to create dataset\n",
    "for rank in ranks:\n",
    "    start = time.time()\n",
    "    #creating list of ids to ensure not saving same replay from perspective of a different player\n",
    "    ids_checked = []\n",
    "    days_checked = 0\n",
    "    main_calls=0\n",
    "    sub_calls=0\n",
    "    limit_reached=False\n",
    "    before_date, after_date, season = next_date(starting_date, season)\n",
    "    print(\"Starting new rank...\")\n",
    "    if ranks_done == len(ranks):\n",
    "        print(\"All ranks completed.\")\n",
    "        break\n",
    "    else:\n",
    "        while limit_reached == False:\n",
    "            time.sleep(2) #stops too many calls being made\n",
    "            resp = requests.get(url=f\"{base_url}/replays\", headers={\"Authorization\": token}, params={\"playlist\": \"ranked-standard\", \"min-rank\": f\"{rank}\", \"max-rank\": f\"{rank}\", \"replay-date-after\": f\"{after_date}\", \"replay-date-before\": f\"{before_date}\", \"count\":\"200\"})\n",
    "            if resp.status_code == 200:\n",
    "                replays = resp.json()\n",
    "                for rep in replays[\"list\"]:\n",
    "                    if sub_calls < 1000:\n",
    "                        time.sleep(0.4)\n",
    "                        di = rep['id']\n",
    "                        if not (di in ids_checked):\n",
    "                            ids_checked.append(di)\n",
    "                            resp_2 = requests.get(url=f\"{base_url}/replays/{di}\", headers={\"Authorization\": token})\n",
    "                            sub_calls+=1\n",
    "                            if resp_2.status_code == 200:\n",
    "                                resp_2 = resp_2.json()\n",
    "                                dummy = sort_and_average(resp_2)\n",
    "                                #removing any rows that contain NaN values\n",
    "                                dummy = dummy.dropna()\n",
    "                                #onyl adding to list if there are still valid rows\n",
    "                                if dummy.shape[0] != 0:\n",
    "                                    #replay is valid, add season column\n",
    "                                    season_list = [season for i in range(dummy.shape[0])]\n",
    "                                    dummy['season'] = season_list\n",
    "                                    replay_list.append(dummy)\n",
    "                    else:\n",
    "                        print('reached limit')\n",
    "                        limit_reached=True\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"{resp.status_code}, call failed.\")\n",
    "                break\n",
    "            #changing dates being searched between\n",
    "            before_date, after_date, season = next_date(after_date, season)\n",
    "            print(sub_calls)\n",
    "        #saving replays to csv file\n",
    "        replays_to_save = pd.concat(replay_list)\n",
    "        replays_to_save.to_csv(f\"MAIN_{rank}_replays.csv\")\n",
    "        print(f\"{rank} done.\")\n",
    "        ranks_done+=1\n",
    "        replay_list = []\n",
    "        end = time.time()\n",
    "        print(f\"Took {(end-start)/60} mins.\")\n",
    "    time.sleep(3600-(end-start)) #pausing for an hour to stop too many calls being made\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e0d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code loads the dataset for the machine learning algorithms and splits it into training and test sets\n",
    "#execute this cell to load the dataset\n",
    "\n",
    "#reading in and formatting replays\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "ranks = [\"gold-1\", \"gold-2\", \"gold-3\", \"platinum-1\", \"platinum-2\", \"platinum-3\", \"diamond-1\", \"diamond-2\", \"diamond-3\", \"champion-1\", \"champion-2\", \"champion-3\", \"grand-champion-1\", \"grand-champion-2\", \"grand-champion-3\", \"supersonic-legend\"]\n",
    "dif_ranks = [\"gold-1\", \"gold-2\", \"gold-3\", \"diamond-1\", \"diamond-2\", \"diamond-3\", \"grand-champion-1\", \"grand-champion-2\", \"grand-champion-3\"]\n",
    "ranks_num = []\n",
    "prop = 0.7\n",
    "infile = open(\"MMR_branges.txt\", \"r+\")\n",
    "base_nums = []\n",
    "for line in infile:\n",
    "    base_nums.append(int((line.split('\\n')[0]).split(',')[1]))\n",
    "for i in range(len(base_nums)-1):\n",
    "    ranks_num.append((base_nums[i]+base_nums[i+1])/2)\n",
    "d = [i for i in range(len(ranks_num))]\n",
    "a, b, c = np.polyfit(d, ranks_num, 2)\n",
    "ranks_num.append(a*(len(ranks_num)**2)+b*len(ranks_num)+c)\n",
    "d.append(len(ranks_num)-1)\n",
    "r_tgt_train = pd.DataFrame([])\n",
    "r_tgt_test = pd.DataFrame([])\n",
    "c_tgt_train = pd.DataFrame([])\n",
    "c_tgt_test = pd.DataFrame([])\n",
    "feat_train = pd.DataFrame([])\n",
    "feat_test = pd.DataFrame([])\n",
    "sparse_feats = pd.DataFrame([])\n",
    "sparse_tgts = pd.DataFrame([])\n",
    "\n",
    "for r in ranks:\n",
    "    temp_csv = pd.read_csv(f\"MAIN_{r}_replays.csv\", dtype=str)\n",
    "    feat_temp = temp_csv.drop(\"Unnamed: 0\", axis=1)\n",
    "    feat_temp = feat_temp.dropna()\n",
    "    #2986 is min number of replays for each rank, this line balances the dataset\n",
    "    feat_temp = feat_temp[:2986]\n",
    "    rows = len(feat_temp.index)\n",
    "    r_list_train = pd.DataFrame([ranks_num[ranks.index(r)] for i in range(int(rows*prop))])\n",
    "    r_list_test = pd.DataFrame([ranks_num[ranks.index(r)] for i in range(rows-int(rows*prop))])\n",
    "    c_list_train = pd.DataFrame(r for i in range(int(prop*rows)))\n",
    "    c_list_test = pd.DataFrame(r for i in range(rows-int(prop*rows)))\n",
    "    feat_train_temp, feat_test_temp = train_test_split(feat_temp, train_size=prop)\n",
    "    r_tgt_train = pd.concat([r_tgt_train, r_list_train])\n",
    "    r_tgt_test = pd.concat([r_tgt_test, r_list_test])\n",
    "    c_tgt_train = pd.concat([c_tgt_train, c_list_train])\n",
    "    c_tgt_test = pd.concat([c_tgt_test, c_list_test])\n",
    "    feat_train = pd.concat([feat_train, feat_train_temp])\n",
    "    feat_test = pd.concat([feat_test, feat_test_temp])\n",
    "    feat_train.reset_index(drop=True, inplace=True)\n",
    "    feat_test.reset_index(drop=True, inplace=True)\n",
    "    c_tgt_train.reset_index(drop=True, inplace=True)\n",
    "    c_tgt_test.reset_index(drop=True, inplace=True)\n",
    "    r_tgt_train.reset_index(drop=True, inplace=True)\n",
    "    r_tgt_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for r in feat_train.index:\n",
    "    feat_train['season'][r] = float(feat_train['season'][r][1])\n",
    "    if 'T' in feat_train['core.mvp'][r]:\n",
    "        feat_train.at[r, 'core.mvp']=1\n",
    "    else:\n",
    "        feat_train.at[r, 'core.mvp']=0\n",
    "    \n",
    "for r in feat_test.index:\n",
    "    feat_test['season'][r] = float(feat_test['season'][r][1])\n",
    "    if 'T' in feat_test['core.mvp'][r]:\n",
    "        feat_test.at[r, 'core.mvp']=1\n",
    "    else:\n",
    "        feat_test.at[r, 'core.mvp']=0\n",
    "\n",
    "feat_train = feat_train.astype(float)\n",
    "feat_test = feat_test.astype(float)\n",
    "\n",
    "#dropping features that don't provide any new information\n",
    "bad_feats = [\"boost.count_collected_big\", \"boost.count_stolen_big\", \"boost.count_collected_small\", \"boost.count_stolen_small\", \"boost.percent_zero_boost\", \"boost.percent_full_boost\", \"boost.percent_boost_0_25\", \"boost.percent_boost_25_50\", \"boost.percent_boost_50_75\", \"boost.percent_boost_75_100\", \"movement.avg_powerslide_duration\", \"movement.percent_slow_speed\", \"movement.percent_boost_speed\", \"movement.percent_supersonic_speed\", \"movement.percent_ground\", \"movement.percent_low_air\", \"movement.percent_high_air\", \"positioning.percent_defensive_third\", \"positioning.percent_offensive_third\", \"positioning.percent_neutral_third\", \"positioning.percent_defensive_half\", \"positioning.percent_offensive_half\", \"positioning.percent_behind_ball\", \"positioning.percent_infront_ball\", \"positioning.percent_most_back\", \"positioning.percent_most_forward\", \"positioning.percent_closest_to_ball\", \"positioning.percent_farthest_from_ball\"]\n",
    "feat_train.drop(columns=bad_feats, inplace=True)\n",
    "feat_test.drop(columns=bad_feats, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c64154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>core.shots</th>\n",
       "      <th>core.shots_against</th>\n",
       "      <th>core.goals</th>\n",
       "      <th>core.goals_against</th>\n",
       "      <th>core.saves</th>\n",
       "      <th>core.assists</th>\n",
       "      <th>core.score</th>\n",
       "      <th>core.mvp</th>\n",
       "      <th>core.shooting_percentage</th>\n",
       "      <th>boost.bpm</th>\n",
       "      <th>...</th>\n",
       "      <th>positioning.time_behind_ball</th>\n",
       "      <th>positioning.time_infront_ball</th>\n",
       "      <th>positioning.time_most_back</th>\n",
       "      <th>positioning.time_most_forward</th>\n",
       "      <th>positioning.goals_against_while_last_defender</th>\n",
       "      <th>positioning.time_closest_to_ball</th>\n",
       "      <th>positioning.time_farthest_from_ball</th>\n",
       "      <th>demo.inflicted</th>\n",
       "      <th>demo.taken</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.840491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.993865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>303.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46.146626</td>\n",
       "      <td>13.976687</td>\n",
       "      <td>20.558282</td>\n",
       "      <td>17.852761</td>\n",
       "      <td>0.368098</td>\n",
       "      <td>20.208589</td>\n",
       "      <td>18.920245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.641711</td>\n",
       "      <td>1.122995</td>\n",
       "      <td>0.320856</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.320856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.288770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.868984</td>\n",
       "      <td>17.854011</td>\n",
       "      <td>16.203209</td>\n",
       "      <td>22.828877</td>\n",
       "      <td>0.320856</td>\n",
       "      <td>25.780749</td>\n",
       "      <td>17.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320856</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.184874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>312.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.252101</td>\n",
       "      <td>15.687395</td>\n",
       "      <td>18.789916</td>\n",
       "      <td>20.403361</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>17.210084</td>\n",
       "      <td>21.949580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.675676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>244.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.415135</td>\n",
       "      <td>14.516757</td>\n",
       "      <td>28.167568</td>\n",
       "      <td>25.394595</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>31.183784</td>\n",
       "      <td>22.767568</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359281</td>\n",
       "      <td>43.473054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>192.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.085629</td>\n",
       "      <td>21.917964</td>\n",
       "      <td>17.514970</td>\n",
       "      <td>24.179641</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>22.544910</td>\n",
       "      <td>18.790419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33435</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.288344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.239264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>455.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37.157669</td>\n",
       "      <td>22.971166</td>\n",
       "      <td>15.515337</td>\n",
       "      <td>25.214724</td>\n",
       "      <td>0.184049</td>\n",
       "      <td>20.760736</td>\n",
       "      <td>18.128834</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33436</th>\n",
       "      <td>0.776699</td>\n",
       "      <td>1.165049</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>131.650485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.011650</td>\n",
       "      <td>11.027184</td>\n",
       "      <td>26.019417</td>\n",
       "      <td>12.097087</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>19.747573</td>\n",
       "      <td>15.941748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33437</th>\n",
       "      <td>0.425532</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>0.141844</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.992908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.333332</td>\n",
       "      <td>405.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.903546</td>\n",
       "      <td>17.770213</td>\n",
       "      <td>19.361702</td>\n",
       "      <td>17.900709</td>\n",
       "      <td>0.141844</td>\n",
       "      <td>18.609929</td>\n",
       "      <td>15.120567</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33438</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>370.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.135238</td>\n",
       "      <td>20.352381</td>\n",
       "      <td>19.180952</td>\n",
       "      <td>19.428571</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>20.590476</td>\n",
       "      <td>18.171429</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33439</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>371.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49.300000</td>\n",
       "      <td>10.933333</td>\n",
       "      <td>18.166667</td>\n",
       "      <td>15.166667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>9.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33440 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       core.shots  core.shots_against  core.goals  core.goals_against  \\\n",
       "0        0.000000            1.840491    0.000000            0.368098   \n",
       "1        0.641711            1.122995    0.320856            0.641711   \n",
       "2        0.168067            0.168067    0.000000            0.168067   \n",
       "3        0.810811            0.324324    0.648649            0.324324   \n",
       "4        0.000000            0.718563    0.000000            0.179641   \n",
       "...           ...                 ...         ...                 ...   \n",
       "33435    0.000000            1.288344    0.000000            0.368098   \n",
       "33436    0.776699            1.165049    0.582524            0.194175   \n",
       "33437    0.425532            2.127660    0.141844            0.425532   \n",
       "33438    0.380952            1.142857    0.000000            0.190476   \n",
       "33439    0.000000            3.333333    0.000000            1.666667   \n",
       "\n",
       "       core.saves  core.assists  core.score  core.mvp  \\\n",
       "0        0.000000      0.000000    6.993865       0.0   \n",
       "1        0.320856      0.000000   78.288770       0.0   \n",
       "2        0.000000      0.000000   22.184874       0.0   \n",
       "3        0.000000      0.000000  101.675676       1.0   \n",
       "4        0.000000      0.359281   43.473054       0.0   \n",
       "...           ...           ...         ...       ...   \n",
       "33435    0.000000      0.000000   27.239264       0.0   \n",
       "33436    0.194175      0.194175  131.650485       1.0   \n",
       "33437    0.283688      0.000000   60.992908       0.0   \n",
       "33438    0.190476      0.000000   29.714286       0.0   \n",
       "33439    1.666667      0.000000  106.666667       0.0   \n",
       "\n",
       "       core.shooting_percentage  boost.bpm  ...  positioning.time_behind_ball  \\\n",
       "0                      0.000000      303.0  ...                     46.146626   \n",
       "1                     50.000000      318.0  ...                     40.868984   \n",
       "2                      0.000000      312.0  ...                     44.252101   \n",
       "3                     80.000000      244.0  ...                     45.415135   \n",
       "4                      0.000000      192.0  ...                     38.085629   \n",
       "...                         ...        ...  ...                           ...   \n",
       "33435                  0.000000      455.0  ...                     37.157669   \n",
       "33436                 75.000000      432.0  ...                     48.011650   \n",
       "33437                 33.333332      405.0  ...                     40.903546   \n",
       "33438                  0.000000      370.0  ...                     39.135238   \n",
       "33439                  0.000000      371.0  ...                     49.300000   \n",
       "\n",
       "       positioning.time_infront_ball  positioning.time_most_back  \\\n",
       "0                          13.976687                   20.558282   \n",
       "1                          17.854011                   16.203209   \n",
       "2                          15.687395                   18.789916   \n",
       "3                          14.516757                   28.167568   \n",
       "4                          21.917964                   17.514970   \n",
       "...                              ...                         ...   \n",
       "33435                      22.971166                   15.515337   \n",
       "33436                      11.027184                   26.019417   \n",
       "33437                      17.770213                   19.361702   \n",
       "33438                      20.352381                   19.180952   \n",
       "33439                      10.933333                   18.166667   \n",
       "\n",
       "       positioning.time_most_forward  \\\n",
       "0                          17.852761   \n",
       "1                          22.828877   \n",
       "2                          20.403361   \n",
       "3                          25.394595   \n",
       "4                          24.179641   \n",
       "...                              ...   \n",
       "33435                      25.214724   \n",
       "33436                      12.097087   \n",
       "33437                      17.900709   \n",
       "33438                      19.428571   \n",
       "33439                      15.166667   \n",
       "\n",
       "       positioning.goals_against_while_last_defender  \\\n",
       "0                                           0.368098   \n",
       "1                                           0.320856   \n",
       "2                                           0.168067   \n",
       "3                                           0.162162   \n",
       "4                                           0.179641   \n",
       "...                                              ...   \n",
       "33435                                       0.184049   \n",
       "33436                                       0.194175   \n",
       "33437                                       0.141844   \n",
       "33438                                       0.190476   \n",
       "33439                                       1.666667   \n",
       "\n",
       "       positioning.time_closest_to_ball  positioning.time_farthest_from_ball  \\\n",
       "0                             20.208589                            18.920245   \n",
       "1                             25.780749                            17.823529   \n",
       "2                             17.210084                            21.949580   \n",
       "3                             31.183784                            22.767568   \n",
       "4                             22.544910                            18.790419   \n",
       "...                                 ...                                  ...   \n",
       "33435                         20.760736                            18.128834   \n",
       "33436                         19.747573                            15.941748   \n",
       "33437                         18.609929                            15.120567   \n",
       "33438                         20.590476                            18.171429   \n",
       "33439                         19.666667                             9.833333   \n",
       "\n",
       "       demo.inflicted  demo.taken  season  \n",
       "0            0.000000    0.000000     9.0  \n",
       "1            0.000000    0.320856     8.0  \n",
       "2            0.000000    0.000000     9.0  \n",
       "3            0.162162    0.000000     9.0  \n",
       "4            0.000000    0.000000     9.0  \n",
       "...               ...         ...     ...  \n",
       "33435        0.552147    0.000000     8.0  \n",
       "33436        0.000000    0.194175     8.0  \n",
       "33437        0.425532    0.425532     8.0  \n",
       "33438        0.190476    0.190476     8.0  \n",
       "33439        0.000000    0.000000     8.0  \n",
       "\n",
       "[33440 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f98a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r_predicts_to_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/f7r0q45d63j8dz4mnxtnpnsc0000gn/T/ipykernel_729/2797756929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mlin_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_tgt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mlin_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mlin_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_predicts_to_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlin_pred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_tgt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlin_train\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mlbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r_predicts_to_acc' is not defined"
     ]
    }
   ],
   "source": [
    "#this cell shows various different ideas experimented with which did not work\n",
    "\n",
    "#since this is a recreation, this cell should not be run since it is unlikely to execute correctly and does not\n",
    "#improve performance\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#list of feats to remove that provide no new info and will likely confuse algorithms\n",
    "bad_feats = [\"boost.count_collected_big\", \"boost.count_stolen_big\", \"boost.count_collected_small\", \"boost.count_stolen_small\", \"boost.percent_zero_boost\", \"boost.percent_full_boost\", \"boost.percent_boost_0_25\", \"boost.percent_boost_25_50\", \"boost.percent_boost_50_75\", \"boost.percent_boost_75_100\", \"movement.avg_powerslide_duration\", \"movement.percent_slow_speed\", \"movement.percent_boost_speed\", \"movement.percent_supersonic_speed\", \"movement.percent_ground\", \"movement.percent_low_air\", \"movement.percent_high_air\", \"positioning.percent_defensive_third\", \"positioning.percent_offensive_third\", \"positioning.percent_neutral_third\", \"positioning.percent_defensive_half\", \"positioning.percent_offensive_half\", \"positioning.percent_behind_ball\", \"positioning.percent_infront_ball\", \"positioning.percent_most_back\", \"positioning.percent_most_forward\", \"positioning.percent_closest_to_ball\", \"positioning.percent_farthest_from_ball\"]\n",
    "\n",
    "#trying random combos to see which features are important\n",
    "\n",
    "import random as rand\n",
    "\n",
    "#splitting features into different categories\n",
    "\n",
    "all_feats = list(feat_train.columns)\n",
    "core_feats = []\n",
    "boost_feats = []\n",
    "move_feats = []\n",
    "pos_feats = []\n",
    "for f in all_feats:\n",
    "    if f.split('.')[0] == 'boost':\n",
    "        boost_feats.append(f)\n",
    "    elif f.split('.')[0] == 'movement':\n",
    "        move_feats.append(f)\n",
    "    elif f.split('.')[0] == 'positioning':\n",
    "        pos_feats.append(f)\n",
    "    else:\n",
    "        core_feats.append(f)\n",
    "        \n",
    "\n",
    "def rand_feats(n, t):\n",
    "    feats_to_keep = []\n",
    "    if t == 'b':\n",
    "        while len(feats_to_keep) != n:\n",
    "            r = rand.randint(0, len(boost_feats)-1)\n",
    "            if not (boost_feats[r] in feats_to_keep):\n",
    "                feats_to_keep.append(boost_feats[r])\n",
    "    elif t == 'c':\n",
    "        while len(inds) != n:\n",
    "            r = rand.randint(0, len(core_feats))\n",
    "            if not (r in inds):\n",
    "                inds.append(r)\n",
    "                feats_to_keep.append(core_feats[r])\n",
    "    elif t == 'm':\n",
    "        while len(inds) != n:\n",
    "            r = rand.randint(0, len(move_feats))\n",
    "            if not (r in inds):\n",
    "                inds.append(r)\n",
    "                feats_to_keep.append(move_feats[r])\n",
    "    elif t == 'p':\n",
    "        while len(inds) != n:\n",
    "            r = rand.randint(0, len(pos_feats))\n",
    "            if not (r in inds):\n",
    "                inds.append(r)\n",
    "                feats_to_keep.append(pos_feats[r])\n",
    "    feats_to_keep = set(feats_to_keep)\n",
    "    feats = set(all_feats)\n",
    "    temp_feats = feats-feats_to_keep\n",
    "    new_train_set = feat_train.drop(columns=temp_feats)\n",
    "    new_test_set = feat_test.drop(columns=temp_feats)\n",
    "    return new_train_set, new_test_set\n",
    "\n",
    "lbest_set = []\n",
    "lbest_acc = 0\n",
    "tries = 100\n",
    "for hmm in range(tries):\n",
    "    train_fts, test_fts = rand_feats(5, 'b')\n",
    "    lin_mod = LinearRegression().fit(train_fts, r_tgt_train)\n",
    "    lin_pred_test = lin_mod.predict(test_fts)\n",
    "    lin_train = round(r_predicts_to_acc(lin_pred_test, r_tgt_test)*100, 2)\n",
    "    if lin_train > lbest_acc:\n",
    "        lbest_acc = lin_train\n",
    "        lbest_set = train_fts\n",
    "print(lbest_acc)\n",
    "print(lbest_set)\n",
    "\n",
    "#shrinking dataset as much as possible\n",
    "#keeping:\n",
    "#boost: bpm, bcpm, avg am, am col big, am col smol, amount used ss, perc 0, perc 100, perc 0-25, perc 25-50, perc 50-75, perc 75-100\n",
    "#\n",
    "#\n",
    "\n",
    "#removing irrelevant features\n",
    "\n",
    "boost_feats_to_drop = [\"boost.amount_collected\", \"boost.amount_stolen\", \"boost.amount_stolen_big\", \"boost.amount_stolen_small\", \"boost.count_collected_big\", \"boost.count_stolen_big\", \"boost.count_collected_small\", \"boost.count_stolen_small\", \"boost.amount_overfill\", \"boost.amount_overfill_stolen\", \"boost.time_zero_boost\", \"boost.time_full_boost\", \"boost.time_boost_0_25\", \"boost.time_boost_25_50\", \"boost.time_boost_50_75\", \"boost.time_boost_75_100\"]\n",
    "core_feats_to_drop = [\"core.assists\", \"core.shots_against\", \"core.goals\", \"core.goals_against\", \"demo.inflicted\", \"demo.taken\", \"core.score\"]\n",
    "movement_feats_to_drop = [\"movement.total_distance\", \"movement.time_supersonic_speed\", \"movement.time_boost_speed\", \"movement.time_slow_speed\", \"movement.time_ground\", \"movement.time_low_air\", \"movement.time_high_air\", \"movement.time_powerslide\", \"movement.count_powerslide\", \"movement.avg_speed_percentage\"]\n",
    "positioning_feats_to_drop = [\"positioning.avg_distance_to_ball\", \"positioning.avg_distance_to_ball_possession\", \"positioning.avg_distance_to_ball_no_possession\", \"positioning.avg_distance_to_mates\", \"positioning.time_defensive_third\", \"positioning.time_neutral_third\", \"positioning.time_offensive_third\", \"positioning.time_defensive_half\", \"positioning.time_offensive_half\", \"positioning.time_behind_ball\", \"positioning.time_infront_ball\", \"positioning.time_most_back\", \"positioning.time_most_forward\", \"positioning.time_closest_to_ball\", \"positioning.time_farthest_from_ball\", \"positioning.percent_behind_ball\", \"positioning.percent_infront_ball\", \"positioning.percent_most_back\", \"positioning.percent_most_forward\", \"positioning.percent_closest_to_ball\", \"positioning.percent_farthest_from_ball\"]\n",
    "\n",
    "feat_train.drop(columns=boost_feats_to_drop+core_feats_to_drop+movement_feats_to_drop+positioning_feats_to_drop, inplace=True)\n",
    "feat_test.drop(columns=boost_feats_to_drop+core_feats_to_drop+movement_feats_to_drop+positioning_feats_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code obtained from https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "#scaling and normalising the data\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "feat_train = scaler.fit_transform(feat_train)\n",
    "feat_test = scaler.transform(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#reducing the dimensionality of the data\n",
    "\n",
    "pca = PCA(svd_solver='full')\n",
    "feat_train = pca.fit_transform(feat_train)\n",
    "feat_test = pca.transform(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d008e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithms to convert predictions to accuracies, should be run before any regression algorithms are trained\n",
    "\n",
    "def ez_r_predicts_to_acc(pred, tgts, alg, data_type):\n",
    "    all_cranks_train = []\n",
    "    all_cranks_test = []\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    corr = 0\n",
    "    pred_cp = list(pred)\n",
    "    tgt_test = list(tgts[0])\n",
    "    for p in range(len(pred)):\n",
    "        if pred[p] >= 475:\n",
    "            if (pred[p] < 535):\n",
    "                pred_cp[p] = ranks_num[0]\n",
    "            elif (pred[p] < 594):\n",
    "                pred_cp[p] = ranks_num[1]            \n",
    "            elif (pred[p] < 655):\n",
    "                pred_cp[p] = ranks_num[2]\n",
    "            elif (pred[p] < 715):\n",
    "                pred_cp[p] = ranks_num[3]\n",
    "            elif (pred[p] < 774):\n",
    "                pred_cp[p] = ranks_num[4]\n",
    "            elif (pred[p] < 835):\n",
    "                pred_cp[p] = ranks_num[5]\n",
    "            elif (pred[p] < 915):\n",
    "                pred_cp[p] = ranks_num[6]\n",
    "            elif (pred[p] < 995):\n",
    "                pred_cp[p] = ranks_num[7]\n",
    "            elif (pred[p] < 1075):\n",
    "                pred_cp[p] = ranks_num[8]\n",
    "            elif (pred[p] < 1195):\n",
    "                pred_cp[p] = ranks_num[9]\n",
    "            elif (pred[p] < 1315):\n",
    "                pred_cp[p] = ranks_num[10]\n",
    "            elif (pred[p] < 1435):\n",
    "                pred_cp[p] = ranks_num[11]\n",
    "            elif (pred[p] < 1575):\n",
    "                pred_cp[p] = ranks_num[12]\n",
    "            elif (pred[p] < 1715):\n",
    "                pred_cp[p] = ranks_num[13]\n",
    "            elif (pred[p] < 1861):\n",
    "                pred_cp[p] = ranks_num[14]\n",
    "            else:\n",
    "                pred_cp[p] = ranks_num[15]\n",
    "        else:\n",
    "            pred_cp[p]=0\n",
    "    for t in range(len(tgt_test)):\n",
    "        #gold\n",
    "        if (((tgt_test[t] >= 505) and (tgt_test[t] <= 624.5)) and ((pred_cp[t] >= 505) and (pred_cp[t] <= 624.5))) or (((tgt_test[t] >= 875) and (tgt_test[t] <= 1035)) and ((pred_cp[t] >= 875) and (pred_cp[t] <= 1035))) or (((tgt_test[t] >= 1505) and (tgt_test[t] <= 1788)) and ((pred_cp[t] >= 1505) and (pred_cp[t] <= 1788))):\n",
    "            corr+=1\n",
    "            if data_type == 'a':\n",
    "                all_cranks_train[alg][ranks_num.index(pred_cp[t])]+=((1/len(pred))*16)\n",
    "            else:\n",
    "                all_cranks_test[alg][ranks_num.index(pred_cp[t])]+=((1/len(pred))*16)\n",
    "    if data_type == 'a':\n",
    "        print(f\"Average is {(sum(all_cranks_train[alg])/len(all_cranks_train[alg]))}\")\n",
    "    else:\n",
    "        print(f\"Average is {(sum(all_cranks_test[alg])/len(all_cranks_test[alg]))}\")        \n",
    "    return corr/len(tgt_test)\n",
    "\n",
    "[ 505. ,  564.5,  624.5,  875. ,  955. , 1035. , 1505. , 1645. , 1788. ]\n",
    "\n",
    "def r_predicts_to_acc(pred, tgts, alg, data_type):\n",
    "    all_cranks_train = []\n",
    "    all_cranks_test = []\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    corr = 0\n",
    "    pred_cp = list(pred)\n",
    "    tgt_test = list(tgts[0])\n",
    "    for p in range(len(pred)):\n",
    "        if pred[p] >= 475:\n",
    "            if (pred[p] < 535):\n",
    "                pred_cp[p] = ranks_num[0]\n",
    "            elif (pred[p] < 594):\n",
    "                pred_cp[p] = ranks_num[1]            \n",
    "            elif (pred[p] < 655):\n",
    "                pred_cp[p] = ranks_num[2]\n",
    "            elif (pred[p] < 715):\n",
    "                pred_cp[p] = ranks_num[3]\n",
    "            elif (pred[p] < 774):\n",
    "                pred_cp[p] = ranks_num[4]\n",
    "            elif (pred[p] < 835):\n",
    "                pred_cp[p] = ranks_num[5]\n",
    "            elif (pred[p] < 915):\n",
    "                pred_cp[p] = ranks_num[6]\n",
    "            elif (pred[p] < 995):\n",
    "                pred_cp[p] = ranks_num[7]\n",
    "            elif (pred[p] < 1075):\n",
    "                pred_cp[p] = ranks_num[8]\n",
    "            elif (pred[p] < 1195):\n",
    "                pred_cp[p] = ranks_num[9]\n",
    "            elif (pred[p] < 1315):\n",
    "                pred_cp[p] = ranks_num[10]\n",
    "            elif (pred[p] < 1435):\n",
    "                pred_cp[p] = ranks_num[11]\n",
    "            elif (pred[p] < 1575):\n",
    "                pred_cp[p] = ranks_num[12]\n",
    "            elif (pred[p] < 1715):\n",
    "                pred_cp[p] = ranks_num[13]\n",
    "            elif (pred[p] < 1861):\n",
    "                pred_cp[p] = ranks_num[14]\n",
    "            else:\n",
    "                pred_cp[p] = ranks_num[15]\n",
    "        else:\n",
    "            pred_cp[p]=0\n",
    "    for t in range(len(tgt_test)):       \n",
    "        if (tgt_test[t] == pred_cp[t]):\n",
    "            corr+=1\n",
    "            if data_type == 'a':\n",
    "                all_cranks_train[alg][ranks_num.index(pred_cp[t])]+=((1/len(pred))*16)\n",
    "            else:\n",
    "                all_cranks_test[alg][ranks_num.index(pred_cp[t])]+=((1/len(pred))*16)\n",
    "    if data_type == 'a':\n",
    "        print(f\"Average is {(sum(all_cranks_train[alg])/len(all_cranks_train[alg]))}\")\n",
    "    else:\n",
    "        print(f\"Average is {(sum(all_cranks_test[alg])/len(all_cranks_test[alg]))}\")        \n",
    "    return corr/len(tgt_test)\n",
    "\n",
    "def c_ranks_right(tgts, pred, alg, data_type):\n",
    "    all_cranks_train = []\n",
    "    all_cranks_test = []\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_train.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    all_cranks_test.append([0]*16)\n",
    "    pred_cp = list(pred)\n",
    "    tgt_test = list(tgts[0])\n",
    "    for t in range(len(tgt_test)):\n",
    "        if (tgt_test[t] == pred_cp[t]):\n",
    "            if data_type == 'a':\n",
    "                all_cranks_train[alg][ranks.index(pred_cp[t])]+=(1/2100)\n",
    "            else:\n",
    "                all_cranks_test[alg][ranks.index(pred_cp[t])]+=(1/900)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b41685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.162e+08, tolerance: 6.370e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average is 0.17658492822966454\n",
      "Average is 0.17578124999999975\n",
      "17.66 17.58\n",
      "0.7432089571751109\n",
      "221.16652212527268\n",
      "172.47526691368694\n"
     ]
    }
   ],
   "source": [
    "#run this code to train and test the lasso model\n",
    "\n",
    "#SelectKBest was attempted but did not improve performance, code used was the same as \n",
    "#the example at https://www.datatechnotes.com/2021/02/seleckbest-feature-selection-example-in-python.html\n",
    "\n",
    "#Regression - LASSO (include r2, mse, mae)\n",
    "\n",
    "alphas = 10**np.linspace(-5,-10,100)*40\n",
    "#based on test, best alpha found is below\n",
    "alph = 0.00015\n",
    "\n",
    "\n",
    "las_model = Lasso(alpha=alph, max_iter=10000).fit(feat_train, r_tgt_train)\n",
    "las_pred_train = las_model.predict(feat_train)\n",
    "las_pred_test = las_model.predict(feat_test)\n",
    "lacc_train = round(r_predicts_to_acc(las_pred_train, r_tgt_train, 0, 'a')*100, 2)\n",
    "lacc_test = round(r_predicts_to_acc(las_pred_test, r_tgt_test, 0, 'b')*100, 2)\n",
    "print(lacc_train, lacc_test)\n",
    "print(r2_score(r_tgt_test, las_pred_test))\n",
    "print(mean_squared_error(r_tgt_test, las_pred_test, squared=False))\n",
    "print(mean_absolute_error(r_tgt_test, las_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2edbd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average is 0.20431082589285676\n",
      "Average is 0.21375598086124295\n",
      "21.38 20.43\n",
      "0.7040464054908577\n",
      "237.43309993396156\n",
      "155.5715447208643\n"
     ]
    }
   ],
   "source": [
    "#run this code to train and test the polynomial regression model\n",
    "\n",
    "#Regression - Polynomial, takes a bit of time to load\n",
    "#code from https://towardsdatascience.com/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degree = 2\n",
    "polyreg = make_pipeline(PolynomialFeatures(degree, include_bias=False, interaction_only=False, order='C'), LinearRegression())\n",
    "polyreg.fit(feat_train, r_tgt_train)\n",
    "p_pred_test = polyreg.predict(feat_test)\n",
    "p_pred_train = polyreg.predict(feat_train)\n",
    "pacc_test = round(r_predicts_to_acc(p_pred_test, r_tgt_test, 1, 'b')*100, 2)\n",
    "pacc_train = round(r_predicts_to_acc(p_pred_train, r_tgt_train, 1, 'a')*100, 2)\n",
    "\n",
    "#p_r2 = r2_score(r_tgt_test, p_pred)\n",
    "#p_mse = mean_squared_error(r_tgt_test, p_pred)\n",
    "#p_mae = mean_absolute_error(r_tgt_test, p_pred)\n",
    "print(pacc_train, pacc_test)\n",
    "print(r2_score(r_tgt_test, p_pred_test))\n",
    "print(mean_squared_error(r_tgt_test, p_pred_test, squared=False))\n",
    "print(mean_absolute_error(r_tgt_test, p_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb0bf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.91 22.42\n",
      "0.2523184918170106\n",
      "0.36759341492944836\n",
      "0.22419084821428573\n"
     ]
    }
   ],
   "source": [
    "#run this cell to train and test the random forest model\n",
    "\n",
    "#Classification - Random Forest\n",
    "\n",
    "#links below used to obtain code and inform methods of improving performance\n",
    "#https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "#https://www.datasciencelearner.com/how-to-improve-accuracy-of-random-forest-classifier/\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#https://www.datacamp.com/tutorial/random-forests-classifier-python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_estimators = [150, 200, 250]\n",
    "max_features = ['sqrt', 'log2']\n",
    "max_depth = [16, 32, None]\n",
    "min_samples_split = [8, 10, 14]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "#reduce feature number\n",
    "best_score = 0\n",
    "best_i = 0\n",
    "rf_model = RandomForestClassifier(n_estimators = 100, max_depth=4, n_jobs=-1, random_state=3)\n",
    "#kf_model_scores = cross_val_score(rf_model, feat_train.append(feat_test), np.ravel(c_tgt_train.append(c_tgt_test)), cv=10)\n",
    "#temp_best = max(kf_model_scores)\n",
    "#print(f\"{temp_best}, {i}, +{round((temp_best-best_score)*100, 2)}%\")\n",
    "#if temp_best > best_score:\n",
    "#    best_i = i\n",
    "#    best_score = temp_best\n",
    "rf_model.fit(feat_train, np.ravel(c_tgt_train))\n",
    "rf_pred_train = rf_model.predict(feat_train)\n",
    "rf_pred_test = rf_model.predict(feat_test)\n",
    "#converting to general labels for comparison to external paper\n",
    "#for i in range(len(rf_pred_train)):\n",
    "#    rf_pred_train[i] = rf_pred_train[i].split('-')[0]\n",
    "#for j in range(len(rf_pred_test)):\n",
    "#    rf_pred_test[j] = rf_pred_test[j].split('-')[0]\n",
    "#for a in range(len(c_tgt_train)):\n",
    "#    c_tgt_train.at[a, 0] = c_tgt_train[0][a].split('-')[0]\n",
    "#for b in range(len(c_tgt_test)):\n",
    "#    c_tgt_test.at[b, 0] = c_tgt_test[0][b].split('-')[0]\n",
    "rfacc_train = round(metrics.accuracy_score(rf_pred_train, c_tgt_train)*100, 2)\n",
    "rfacc_test = round(metrics.accuracy_score(rf_pred_test, c_tgt_test)*100, 2)\n",
    "#c_ranks_right(c_tgt_train, rf_pred_train, 2, 'a')\n",
    "#c_ranks_right(c_tgt_test, rf_pred_test, 2, 'b')\n",
    "print(rfacc_train, rfacc_test)\n",
    "print(metrics.f1_score(rf_pred_test, c_tgt_test, average='weighted'))\n",
    "print(metrics.precision_score(rf_pred_test, c_tgt_test, average='weighted'))\n",
    "print(metrics.recall_score(rf_pred_test, c_tgt_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755f74f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/f7r0q45d63j8dz4mnxtnpnsc0000gn/T/ipykernel_1423/3328767622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mc_tgt_test_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_tgt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi:softprob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exact'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_tgt_train_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mx_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mx_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1514\u001b[0m             )\n\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1517\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run this cell to train and test the xgboost model\n",
    "\n",
    "#code obtained from https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn/notebook\n",
    "#Classification - XGBoost \n",
    "#exact is best but takes a long time, use hist for quicker results   \n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "#reduce tree depth\n",
    "#reduce number of trees\n",
    "#lower learning rate\n",
    "#change objective?\n",
    "#reduce number of features\n",
    "start = time.time()\n",
    "enc = preprocessing.LabelEncoder()\n",
    "xg_tgt_train = np.ravel(c_tgt_train)\n",
    "xg_tgt_test = np.ravel(c_tgt_test)\n",
    "enc.fit(xg_tgt_train)\n",
    "enc.fit(xg_tgt_test)\n",
    "c_tgt_train_enc = enc.transform(xg_tgt_train)\n",
    "c_tgt_test_enc = enc.transform(xg_tgt_test)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=150, max_depth=12, eta=0.01, objective=\"multi:softprob\", tree_method='exact')\n",
    "xgb_model.fit(feat_train, c_tgt_train_enc)\n",
    "x_pred_train = xgb_model.predict(feat_train)\n",
    "x_pred_train = enc.inverse_transform(x_pred_train)\n",
    "x_pred_test = xgb_model.predict(feat_test)\n",
    "x_pred_test = enc.inverse_transform(x_pred_test)\n",
    "xacc_train = round(metrics.accuracy_score(x_pred_train, c_tgt_train)*100, 2)\n",
    "xacc_test = round(metrics.accuracy_score(x_pred_test, c_tgt_test)*100, 2)\n",
    "#c_ranks_right(c_tgt_train, x_pred_train, 3, 'a')\n",
    "#c_ranks_right(c_tgt_test, x_pred_test, 3, 'b')\n",
    "print(xacc_train, xacc_test)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(metrics.f1_score(x_pred_test, c_tgt_test, average='weighted'))\n",
    "print(metrics.precision_score(x_pred_test, c_tgt_test, average='weighted'))\n",
    "print(metrics.recall_score(x_pred_test, c_tgt_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef6f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.14 21.51 7.05480231071863e-11\n",
      "0.23403711055641493\n",
      "0.28264298730966997\n",
      "0.21505301339285715\n"
     ]
    }
   ],
   "source": [
    "#run this cell to train and test the naive bayes model\n",
    "\n",
    "#code from https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "#Classification - Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "als = 10**np.linspace(-9,-15,100)\n",
    "al = 7.05480231071863e-11 #best value found\n",
    "\n",
    "nb_model = GaussianNB(var_smoothing=al)\n",
    "nb_model.fit(feat_train, np.ravel(c_tgt_train))\n",
    "nb_pred_train = nb_model.predict(feat_train)\n",
    "nb_pred_test = nb_model.predict(feat_test)\n",
    "nbacc_train = round(metrics.accuracy_score(nb_pred_train, c_tgt_train)*100, 2)\n",
    "nbacc_test = round(metrics.accuracy_score(nb_pred_test, c_tgt_test)*100, 2)\n",
    "#c_ranks_right(c_tgt_train, nb_pred_train, 4, 'a')\n",
    "#c_ranks_right(c_tgt_test, nb_pred_test, 4, 'b')\n",
    "print(nbacc_train, nbacc_test, al)\n",
    "print(metrics.f1_score(nb_pred_test, c_tgt_test, average='weighted'))\n",
    "print(metrics.precision_score(nb_pred_test, c_tgt_test, average='weighted'))\n",
    "print(metrics.recall_score(nb_pred_test, c_tgt_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553809cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844ad88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell reads in the dataset from memory and should be\n",
    "#exectued before any training is done\n",
    "\n",
    "#reading in and formatting replays\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ranks = [\"gold-1\", \"gold-2\", \"gold-3\", \"platinum-1\", \"platinum-2\", \"platinum-3\", \"diamond-1\", \"diamond-2\", \"diamond-3\", \"champion-1\", \"champion-2\", \"champion-3\", \"grand-champion-1\", \"grand-champion-2\", \"grand-champion-3\", \"supersonic-legend\"]\n",
    "dif_ranks = [\"gold-1\", \"gold-2\", \"gold-3\", \"diamond-1\", \"diamond-2\", \"diamond-3\", \"grand-champion-1\", \"grand-champion-2\", \"grand-champion-3\"]\n",
    "ranks_num = []\n",
    "infile = open(\"MMR_branges.txt\", \"r+\")\n",
    "base_nums = []\n",
    "for line in infile:\n",
    "    base_nums.append(int((line.split('\\n')[0]).split(',')[1]))\n",
    "infile.close\n",
    "for i in range(len(base_nums)-1):\n",
    "    ranks_num.append((base_nums[i]+base_nums[i+1])/2)\n",
    "d = [i for i in range(len(ranks_num))]\n",
    "a, b, c = np.polyfit(d, ranks_num, 2)\n",
    "ranks_num.append(a*(len(ranks_num)**2)+b*len(ranks_num)+c)\n",
    "d.append(len(ranks_num)-1)\n",
    "r_tgt_train = pd.DataFrame([])\n",
    "r_tgt_test = pd.DataFrame([])\n",
    "c_tgt_train = pd.DataFrame([])\n",
    "c_tgt_test = pd.DataFrame([])\n",
    "feat_train = pd.DataFrame([])\n",
    "feat_test = pd.DataFrame([])\n",
    "\n",
    "for r in ranks:\n",
    "    temp_csv = pd.read_csv(f\"MAIN_{r}_replays.csv\", dtype=str)\n",
    "    feat_temp = temp_csv.drop(\"Unnamed: 0\", axis=1)\n",
    "    feat_temp = feat_temp.dropna()\n",
    "    #2986 is min number of replays for each rank, this line balances the dataset\n",
    "    feat_temp = feat_temp[:2986]\n",
    "    rows = len(temp_csv.index)\n",
    "    r_list_train = pd.DataFrame([ranks_num[ranks.index(r)] for i in range(int(rows*0.7))])\n",
    "    r_list_test = pd.DataFrame([ranks_num[ranks.index(r)] for i in range(rows-int(rows*0.7))])\n",
    "    c_list_train = pd.DataFrame(r for i in range(int(rows*0.7)))\n",
    "    c_list_test = pd.DataFrame(r for i in range(rows-int(rows*0.7)))\n",
    "    feat_train_temp, feat_test_temp = train_test_split(feat_temp, train_size=0.7, random_state=1)\n",
    "    r_tgt_train = pd.concat([r_tgt_train, r_list_train])\n",
    "    r_tgt_test = pd.concat([r_tgt_test, r_list_test])\n",
    "    c_tgt_train = pd.concat([c_tgt_train, c_list_train])\n",
    "    c_tgt_test = pd.concat([c_tgt_test, c_list_test])\n",
    "    feat_train = pd.concat([feat_train, feat_train_temp])\n",
    "    feat_test = pd.concat([feat_test, feat_test_temp])\n",
    "    feat_train.reset_index(drop=True, inplace=True)\n",
    "    feat_test.reset_index(drop=True, inplace=True)\n",
    "    c_tgt_train.reset_index(drop=True, inplace=True)\n",
    "    c_tgt_test.reset_index(drop=True, inplace=True)\n",
    "    r_tgt_train.reset_index(drop=True, inplace=True)\n",
    "    r_tgt_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for r in feat_train.index:\n",
    "    feat_train['season'][r] = float(feat_train['season'][r][1])\n",
    "    if 'T' in feat_train['core.mvp'][r]:\n",
    "        feat_train.at[r, 'core.mvp']=1\n",
    "    else:\n",
    "        feat_train.at[r, 'core.mvp']=0\n",
    "    \n",
    "for r in feat_test.index:\n",
    "    feat_test['season'][r] = float(feat_test['season'][r][1])\n",
    "    if 'T' in feat_test['core.mvp'][r]:\n",
    "        feat_test.at[r, 'core.mvp']=1\n",
    "    else:\n",
    "        feat_test.at[r, 'core.mvp']=0\n",
    "\n",
    "feat_train = feat_train.astype(float)\n",
    "feat_test = feat_test.astype(float)\n",
    "\n",
    "#dropping features that don't provide any new information\n",
    "bad_feats = [\"boost.count_collected_big\", \"boost.count_stolen_big\", \"boost.count_collected_small\", \"boost.count_stolen_small\", \"boost.percent_zero_boost\", \"boost.percent_full_boost\", \"boost.percent_boost_0_25\", \"boost.percent_boost_25_50\", \"boost.percent_boost_50_75\", \"boost.percent_boost_75_100\", \"movement.avg_powerslide_duration\", \"movement.percent_slow_speed\", \"movement.percent_boost_speed\", \"movement.percent_supersonic_speed\", \"movement.percent_ground\", \"movement.percent_low_air\", \"movement.percent_high_air\", \"positioning.percent_defensive_third\", \"positioning.percent_offensive_third\", \"positioning.percent_neutral_third\", \"positioning.percent_defensive_half\", \"positioning.percent_offensive_half\", \"positioning.percent_behind_ball\", \"positioning.percent_infront_ball\", \"positioning.percent_most_back\", \"positioning.percent_most_forward\", \"positioning.percent_closest_to_ball\", \"positioning.percent_farthest_from_ball\"]\n",
    "feat_train.drop(columns=bad_feats, inplace=True)\n",
    "feat_test.drop(columns=bad_feats, inplace=True)\n",
    "\n",
    "feat_train['rank'] = c_tgt_train\n",
    "feat_test['rank'] = c_tgt_test\n",
    "\n",
    "#manually shuffling features before model is trained\n",
    "feat_test = feat_test.sample(frac=1).reset_index()\n",
    "feat_train = feat_train.sample(frac=1).reset_index()\n",
    "\n",
    "#re-splitting dataframe into features and target labels after shuffling\n",
    "c_tgt_train = pd.DataFrame(feat_train['rank'])\n",
    "feat_train.drop(columns=['index', 'rank'], inplace=True)\n",
    "c_tgt_test = pd.DataFrame(feat_test['rank'])\n",
    "feat_test.drop(columns=['index', 'rank'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83fe435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell scales the data\n",
    "#this speeds up the training process and should be run before the dataset\n",
    "#the dataset is turned into a dataloader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train = pd.DataFrame(scaler.fit_transform(feat_train))\n",
    "feat_test = pd.DataFrame(scaler.transform(feat_test))\n",
    "\n",
    "#pca = PCA()\n",
    "#feat_train = pd.DataFrame(pca.fit_transform(feat_train))\n",
    "#feat_test = pd.DataFrame(pca.transform(feat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e210a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell contains the different network architectures attempted and\n",
    "#converts the dataset from a dataframe into a dataloader\n",
    "#run this cell with either SimpleNetwork or SimpleConvNetwork before\n",
    "#training either model\n",
    "\n",
    "#this code is based on https://github.com/atapour/dl-pytorch/blob/main/4.Classifier/4_PyTorch_Programming_Classifier.ipynb, which is released under the LGPL License\n",
    "#conv code from https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from livelossplot import PlotLosses\n",
    "import os\n",
    "\n",
    "ranks = ['gold-1', 'gold-2', 'gold-3', 'platinum-1', 'platinum-2', 'platinum-3', 'diamond-1', 'diamond-2', 'diamond-3', 'champion-1', 'champion-2', 'champion-3', 'grand-champion-1', 'grand-champion-2', 'grand-champion-3', 'supersonic-legend']\n",
    "ranks_dict = {}\n",
    "for val, rank in enumerate(ranks):\n",
    "    ranks_dict[rank] = val\n",
    "\n",
    "def to_tensor_loader(train, test):\n",
    "    train = torch.tensor(train.values)\n",
    "    test = torch.tensor(test.values)\n",
    "    train = DataLoader(train, batch_size=256, drop_last=True)\n",
    "    test = DataLoader(test, batch_size=256, drop_last=True)\n",
    "    return train, test\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "#converting tgt dataframes to tensors\n",
    "t_train_tgt = torch.zeros(len(feat_train))\n",
    "t_test_tgt = torch.zeros(len(feat_test))\n",
    "for i in range(len(feat_train)):\n",
    "    t_train_tgt[i] = ranks_dict[c_tgt_train['rank'][i]]\n",
    "for i in range(len(feat_test)):\n",
    "    t_test_tgt[i] = ranks_dict[c_tgt_test['rank'][i]]\n",
    "            \n",
    "train_loader, test_loader = to_tensor_loader(feat_train, feat_test)\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "#defining simple network and optimiser\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(57, 47)\n",
    "        self.l2 = nn.Linear(47, 37)\n",
    "        self.l3 = nn.Linear(37, 27)\n",
    "        self.l4 = nn.Linear(27, 16)\n",
    "        self.do = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
    "        self.do\n",
    "        x = self.l2(x)\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
    "        self.do\n",
    "        x = self.l3(x)\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
    "        self.do\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleConvNetworkSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNetworkSeq, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(56, 16)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "                \n",
    "        \n",
    "class SimpleConvNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNetwork, self).__init__()\n",
    "        self.l1 = nn.Conv1d(1, 4, kernel_size=3, stride=1, padding=1) \n",
    "        self.l2 = nn.BatchNorm1d(4) \n",
    "        self.l3 = nn.Conv1d(4, 8, kernel_size=3, stride=1, padding=1) \n",
    "        self.l4 = nn.BatchNorm1d(8)\n",
    "        self.do = nn.Dropout(0.2)\n",
    "        self.l5 = nn.Linear(456, 228)\n",
    "        self.l6 = nn.Linear(228, 114)\n",
    "        self.l7 = nn.Linear(114, 57)\n",
    "        self.l8 = nn.Linear(57, 16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        self.do\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.l5(x)\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = self.l6(x)\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = self.l7(x)\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = self.l8(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device('cpu')\n",
    "model = SimpleNetwork().to(device)\n",
    "#model = SimpleConvNetwork().to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "liveloss=PlotLosses()\n",
    "logs={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dd667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12.3498 Test: 12.207 Train Loss: 2.5362 Test Loss: 2.5486 Epoch: 20\n",
      "Train: 15.5829 Test: 15.8482 Train Loss: 2.296 Test Loss: 2.3049 Epoch: 40\n",
      "Train: 18.2091 Test: 18.8267 Train Loss: 2.1831 Test Loss: 2.1809 Epoch: 60\n",
      "Train: 20.8474 Test: 21.1984 Train Loss: 2.094 Test Loss: 2.0752 Epoch: 80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/f7r0q45d63j8dz4mnxtnpnsc0000gn/T/ipykernel_3818/579726841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mall_tgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# inner loop - iterating over the batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# getting the input and the ground truth labels from the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#code below is used to train the neural network and monitor its performance\n",
    "#run this cell to do this\n",
    "\n",
    "#using optuna to improve performance https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837\n",
    "import time\n",
    "#import optuna\n",
    "start = time.time()\n",
    "\n",
    "#def objective(trial):\n",
    "#    params = {\n",
    "#        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "#        'optimiser': trial.suggest_categorical('optimiser', [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
    "#        'n_unit': trial.suggest_int(\"n_unit\", 28, 42)\n",
    "#    }\n",
    "#    model=build_model(params)\n",
    "#    accuracy = train_and_evaluate(params, model)\n",
    "#    return accuracy\n",
    "\n",
    "#def build_model(params):\n",
    "#    in_features=57\n",
    "#    return nn.Sequential(\n",
    "#        nn.Linear(in_features, params['n_unit']),\n",
    "#        nn.LeakyReLU(),\n",
    "#        nn.Linear(params['n_unit'], 16),\n",
    "#        nn.LeakyReLU()\n",
    "#    )\n",
    "\n",
    "#def train_and_evaluate(param, model):\n",
    "\n",
    "# outer loop - going over the steps we want to train for\n",
    "step=0\n",
    "while step <= 2000:\n",
    "    train_accuracy=0\n",
    "    all_preds = []\n",
    "    all_tgts = []\n",
    "    # inner loop - iterating over the batches\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        # getting the input and the ground truth labels from the training set\n",
    "        x, gt = batch, t_train_tgt[(i*256):((i+1)*256)]\n",
    "        x, gt = x.to(device), gt.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = x.to(torch.float32)\n",
    "        #x.unsqueeze_(1)\n",
    "        output = model(x)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(output, gt.long())\n",
    "        #optmiser = getattr(torch.optim, param['optimiser'])(model.parameters(), lr=param['learning_rate'])\n",
    "\n",
    "        # explicitly set the gradients to zero before backpropagation\n",
    "        model.zero_grad()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # calculating the accuracy\n",
    "        _, argmax = torch.max(output, dim=1)\n",
    "        train_corr = argmax.eq(gt).float().mean() * 256\n",
    "        train_accuracy+=train_corr\n",
    "    \n",
    "    train_accuracy/=(len(feat_train)-(len(feat_train)%256))\n",
    "    train_accuracy*=100\n",
    "    step+=1\n",
    "    # every k steps we evaluate the model on the test set\n",
    "    k=20\n",
    "    if step % k == 0:\n",
    "        test_accuracy=0\n",
    "        # when we test, we don't need gradients\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # beginning the loop for evaluation:\n",
    "            for j, test_batch in enumerate(test_loader):\n",
    "                # getting the input and the ground truth labels from the training set\n",
    "                x, gt = test_batch, t_test_tgt[j*256:((j+1)*256)]\n",
    "                x, gt = x.to(device), gt.to(device)\n",
    "\n",
    "                # forward pass\n",
    "                x = x.view(x.size(0), -1)\n",
    "                x = x.to(torch.float32)\n",
    "                #x.unsqueeze_(1)\n",
    "                output = model(x)\n",
    "\n",
    "                # test loss\n",
    "                test_loss = criterion(output, gt.long())\n",
    "\n",
    "                # calculating the test accuracy\n",
    "                _, argmax = torch.max(output, dim=1)\n",
    "                test_corr = argmax.eq(gt).float().mean() * 256\n",
    "                test_accuracy+=test_corr\n",
    "\n",
    "            test_accuracy/=(len(feat_test)-(len(feat_test)%256))\n",
    "            test_accuracy*=100\n",
    "\n",
    "            # logging train and test losses and accuracies\n",
    "            logs['Loss'] = loss.item()\n",
    "            logs['val_Loss'] = test_loss.item()\n",
    "            logs['Accuracy'] = train_accuracy\n",
    "            logs['val_Accuracy'] = test_accuracy\n",
    "            print(f\"Train: {round(float(train_accuracy), 4)} Test: {round(float(test_accuracy), 4)} Train Loss: {round(float(loss.item()), 4)} Test Loss: {round(float(test_loss.item()), 4)} Epoch: {step}\")\n",
    "            #liveloss.update(logs)\n",
    "            #liveloss.send()\n",
    "    #return test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "#study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "#study.optimize(objective, n_trials=30)\n",
    "end = time.time()\n",
    "overall = (end-start)/60\n",
    "print(f\"Took {overall} mins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b9ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
